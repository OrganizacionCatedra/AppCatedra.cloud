'use server';
/**
 * @fileOverview A voice assistant flow that handles speech-to-text, calls the main assistant, and converts the response to speech.
 */
import { ai } from '@/ai/genkit';
import { z } from 'genkit';
import { assistantFlow } from './assistant-flow';
import { productCategories } from '@/lib/products';
import { plans } from '@/lib/plans';
import wav from 'wav';
import type { AssistantInput } from '@/lib/types';
import { googleAI } from '@genkit-ai/googleai';

// Input: Base64-encoded audio data URI
const VoiceInputSchema = z.string();

// Output: The transcribed text and the synthesized audio response
const VoiceOutputSchema = z.object({
  text: z.object({
    userInput: z.string(),
    modelResponse: z.string(),
  }),
  audio: z.string().optional(),
});

async function toWav(
  pcmData: Buffer,
  channels = 1,
  rate = 24000,
  sampleWidth = 2
): Promise<string> {
  return new Promise((resolve, reject) => {
    const writer = new wav.Writer({
      channels,
      sampleRate: rate,
      bitDepth: sampleWidth * 8,
    });

    const bufs: any[] = [];
    writer.on('error', reject);
    writer.on('data', (d) => {
      bufs.push(d);
    });
    writer.on('end', () => {
      resolve(Buffer.concat(bufs).toString('base64'));
    });

    writer.write(pcmData);
    writer.end();
  });
}

const voiceAssistantFlow = ai.defineFlow(
  {
    name: 'voiceAssistantFlow',
    inputSchema: VoiceInputSchema,
    outputSchema: VoiceOutputSchema,
  },
  async (audioDataUri) => {
    // 1. Speech-to-Text
    const sttResponse = await ai.generate({
      model: googleAI.model('gemini-1.5-flash-latest'),
      prompt: [
        { text: 'Transcribe el siguiente audio:' },
        { media: { url: audioDataUri } },
      ],
    });
    const userInput = sttResponse.text;

    // 2. Get assistant's text response (using the existing flow)
    const assistantInput: AssistantInput = {
      history: [{ role: 'user', content: userInput }], // For now, we use a simple history
      context: {
        products: productCategories,
        plans,
      },
    };
    const modelResponse = await assistantFlow(assistantInput);

    // 3. Text-to-Speech
    const ttsResponse = await ai.generate({
      model: googleAI.model('gemini-2.5-flash-preview-tts'),
      config: {
        responseModalities: ['AUDIO'],
        speechConfig: {
          voiceConfig: {
            prebuiltVoiceConfig: { voiceName: 'Algenib' }, // A standard male voice
          },
        },
      },
      prompt: modelResponse,
    });
    
    const ttsAudio = ttsResponse.media;

    if (!ttsAudio) {
      throw new Error('No audio was generated by the TTS model.');
    }
    
    // The audio is PCM, we need to convert it to WAV for browser compatibility
    const audioBuffer = Buffer.from(
      ttsAudio.url.substring(ttsAudio.url.indexOf(',') + 1),
      'base64'
    );
    const wavBase64 = await toWav(audioBuffer);
    const wavDataUri = `data:audio/wav;base64,${wavBase64}`;


    return {
      text: {
        userInput: userInput,
        modelResponse: modelResponse,
      },
      audio: wavDataUri,
    };
  }
);


export async function askAssistantVoice(audioDataUri: string): Promise<z.infer<typeof VoiceOutputSchema>> {
  return await voiceAssistantFlow(audioDataUri);
}
